{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.models import ResNet50_Weights\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\"\n",
    "            )\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze all the parameters in the model\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer to match the number of classes\n",
    "num_features = resnet50.fc.in_features\n",
    "resnet50.fc = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: Counter({0: 62920, 1: 44509})\n",
      "Validation class counts: Counter({0: 13482, 1: 9537})\n",
      "Limited Train class counts: Counter({0: 22254, 1: 22254})\n",
      "Limited Validation class counts: Counter({0: 4768, 1: 4768})\n"
     ]
    }
   ],
   "source": [
    "# Load the full training and validation datasets\n",
    "train_dataset_full = ImageFolder(root=\"../data/processed/train\", transform=transform)\n",
    "val_dataset_full = ImageFolder(root=\"../data/processed/val\", transform=transform)\n",
    "\n",
    "\n",
    "# Function to count the number of images in each class\n",
    "def count_images_per_class(dataset):\n",
    "    class_counts = Counter()\n",
    "    for _, label in dataset.samples:\n",
    "        class_counts[label] += 1\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "# Count the number of images in each class for the full datasets\n",
    "train_class_counts = count_images_per_class(train_dataset_full)\n",
    "val_class_counts = count_images_per_class(val_dataset_full)\n",
    "\n",
    "# Print the class distributions for the full datasets\n",
    "print(f\"Train class counts: {train_class_counts}\")\n",
    "print(f\"Validation class counts: {val_class_counts}\")\n",
    "\n",
    "\n",
    "# Function to balance the dataset by limiting the number of images in each class\n",
    "def balance_dataset(dataset, limit=0.1):\n",
    "    # Get indices for each class\n",
    "    class_indices = {\n",
    "        cls: np.where(np.array(dataset.targets) == cls)[0]\n",
    "        for cls in range(len(dataset.classes))\n",
    "    }\n",
    "    # Determine the number of images to keep per class\n",
    "    min_class_count = int(\n",
    "        min(len(indices) for indices in class_indices.values()) * limit\n",
    "    )\n",
    "\n",
    "    balanced_indices = []\n",
    "    # Randomly select indices to balance the dataset\n",
    "    for indices in class_indices.values():\n",
    "        balanced_indices.extend(\n",
    "            np.random.choice(indices, min_class_count, replace=False)\n",
    "        )\n",
    "\n",
    "    # Return a subset of the dataset with balanced classes\n",
    "    return Subset(dataset, balanced_indices)\n",
    "\n",
    "\n",
    "# Balance the training and validation datasets\n",
    "train_dataset = balance_dataset(train_dataset_full, limit=0.5)\n",
    "val_dataset = balance_dataset(val_dataset_full, limit=0.5)\n",
    "\n",
    "# Create DataLoaders for the balanced datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Function to count the number of images in each class for a subset\n",
    "def count_images_per_class_subset(subset):\n",
    "    class_counts = Counter()\n",
    "    for idx in subset.indices:\n",
    "        _, label = subset.dataset.samples[idx]\n",
    "        class_counts[label] += 1\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "# Count the number of images in each class for the balanced subsets\n",
    "limited_train_class_counts = count_images_per_class_subset(train_dataset)\n",
    "limited_val_class_counts = count_images_per_class_subset(val_dataset)\n",
    "\n",
    "# Print the class distributions for the balanced subsets\n",
    "print(f\"Limited Train class counts: {limited_train_class_counts}\")\n",
    "print(f\"Limited Validation class counts: {limited_val_class_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the device, loss criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "resnet50 = resnet50.to(device)\n",
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(resnet50.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for saving checkpoints and final models\n",
    "checkpoint_dir = \"checkpoints/ResNet/\"\n",
    "saved_model_dir = \"saved_models/ResNet/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "\n",
    "# Define paths for the checkpoint and final model\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
    "saved_model_path = os.path.join(saved_model_dir, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=25,\n",
    "    patience=5,\n",
    "    checkpoint_path=\"checkpoint.pth\",\n",
    "):\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[\"train\"].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[\"train\"].dataset)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\"\n",
    "        )\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders[\"val\"]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(dataloaders[\"val\"].dataset)\n",
    "        val_epoch_acc = val_running_corrects.double() / len(dataloaders[\"val\"].dataset)\n",
    "\n",
    "        print(\n",
    "            f\"Validation Loss: {val_epoch_loss:.4f}, Validation Acc: {val_epoch_acc:.4f}\"\n",
    "        )\n",
    "        history[\"val_loss\"].append(val_epoch_loss)\n",
    "        history[\"val_acc\"].append(val_epoch_acc)\n",
    "\n",
    "        # Early stopping and checkpoint saving\n",
    "        early_stopping(val_epoch_loss, model, checkpoint_path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Define dataloaders dictionary\n",
    "dataloaders = {\"train\": train_loader, \"val\": val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "    resnet50,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=10,\n",
    "    patience=5,\n",
    "    checkpoint_path=checkpoint_path,\n",
    ")\n",
    "\n",
    "\n",
    "# Saving the final model\n",
    "torch.save(model.state_dict(), saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m history_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(saved_model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_history.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save history\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m save_history(\u001b[43mhistory\u001b[49m, history_path)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load history\u001b[39;00m\n\u001b[1;32m     20\u001b[0m history \u001b[38;5;241m=\u001b[39m load_history(history_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Functions to save and load training history\n",
    "def load_history(history_path):\n",
    "    with open(history_path, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "    return history\n",
    "\n",
    "\n",
    "def save_history(history, history_path):\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "\n",
    "# Define path for saving history\n",
    "history_path = os.path.join(saved_model_dir, \"training_history.json\")\n",
    "\n",
    "# Save history\n",
    "save_history(history, history_path)\n",
    "\n",
    "# Load history\n",
    "history = load_history(history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training and validation loss\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
